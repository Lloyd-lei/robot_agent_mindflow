"""
æ··åˆæ¶æ„ Agent - OpenAIåŸç”ŸFunction Calling + LangChainå·¥å…·æ±  + KV Cacheä¼˜åŒ–

æ ¸å¿ƒç‰¹æ€§:
1. ä½¿ç”¨OpenAIåŸç”ŸAPIè·å¾—å®Œå…¨æ§åˆ¶æƒ(tool_choiceæ§åˆ¶)
2. ä¿ç•™LangChainä¸°å¯Œçš„å·¥å…·ç”Ÿæ€
3. KV Cacheè‡ªåŠ¨ä¼˜åŒ–(ç³»ç»Ÿæç¤ºè¯ç¼“å­˜ã€å¯¹è¯å†å²ç¼“å­˜)
4. 100%å¯é çš„å·¥å…·è°ƒç”¨
5. å®Œæ•´çš„æ¨ç†è¿‡ç¨‹å±•ç¤º
"""
from openai import OpenAI
from typing import List, Dict, Any, Optional
import json
from datetime import datetime

from src.core.agents.base import BaseAgent, AgentResponse
from src.core.config import settings
from src.core.tools.base import BaseTool


class HybridReasoningAgent(BaseAgent):
    """
    æ··åˆæ¶æ„æ¨ç†Agent

    æ¶æ„:
    - OpenAIåŸç”ŸAPI: æ¨ç†å¼•æ“(å¯æ§ã€å¿«é€Ÿã€æ”¯æŒKV Cache)
    - LangChainå·¥å…·: æ‰§è¡Œå¼•æ“(ä¸°å¯Œã€æ˜“æ‰©å±•)
    - KV Cache: æ€§èƒ½ä¼˜åŒ–(å¯¹è¯å†å²ã€ç³»ç»Ÿæç¤ºè¯è‡ªåŠ¨ç¼“å­˜)
    """

    def __init__(
        self,
        tools: List[BaseTool],
        api_key: Optional[str] = None,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        enable_cache: bool = True,
        name: str = "HybridAgent"
    ):
        """
        åˆå§‹åŒ–æ··åˆæ¶æ„Agent

        Args:
            tools: å·¥å…·åˆ—è¡¨
            api_key: OpenAI APIå¯†é’¥(é»˜è®¤ä»é…ç½®è¯»å–)
            model: æ¨¡å‹åç§°(é»˜è®¤ä»é…ç½®è¯»å–)
            temperature: æ¸©åº¦å‚æ•°(é»˜è®¤ä»é…ç½®è¯»å–)
            enable_cache: æ˜¯å¦å¯ç”¨å¯¹è¯å†å²ç¼“å­˜(KV Cacheä¼˜åŒ–)
            name: Agentåç§°
        """
        super().__init__(name=name)

        # é…ç½®
        self.api_key = api_key or settings.openai_api_key
        self.model = model or settings.llm_model
        self.temperature = temperature if temperature is not None else settings.temperature
        self.enable_cache = enable_cache

        # OpenAIå®¢æˆ·ç«¯
        self.client = OpenAI(api_key=self.api_key)

        # å·¥å…·ç®¡ç†
        self.tools = tools
        self.openai_tools = self._convert_tools_to_openai_format()
        self.tool_map = {tool.name: tool for tool in tools}

        # ç³»ç»Ÿæç¤ºè¯(ä¼šè¢«KV Cacheç¼“å­˜,èŠ‚çœæˆæœ¬)
        self.system_prompt = self._create_system_prompt()

        print(f"âœ… æ··åˆæ¶æ„Agentåˆå§‹åŒ–æˆåŠŸ")
        print(f"   å¼•æ“: OpenAIåŸç”ŸAPI ({self.model})")
        print(f"   å·¥å…·: LangChainå·¥å…·æ±  ({len(self.tools)}ä¸ª)")
        print(f"   KV Cache: {'å¯ç”¨' if self.enable_cache else 'ç¦ç”¨'}")
        print(f"   æ¸©åº¦: {self.temperature}")
        print()

    def _create_system_prompt(self) -> str:
        """
        åˆ›å»ºç³»ç»Ÿæç¤ºè¯
        æ³¨æ„: è¿™ä¸ªæç¤ºè¯ä¼šè¢«OpenAIè‡ªåŠ¨ç¼“å­˜(Prompt Caching),èŠ‚çœ50%æˆæœ¬
        """
        # åŠ¨æ€ç”Ÿæˆå·¥å…·åˆ—è¡¨
        tool_descriptions = []
        for tool in self.tools:
            tool_descriptions.append(f"- {tool.name}: {tool.description}")

        tools_text = "\n".join(tool_descriptions)

        return f"""ä½ æ˜¯ä¸€ä¸ªå…·æœ‰å¼ºå¤§æ¨ç†èƒ½åŠ›çš„AIåŠ©æ‰‹ã€‚

ğŸ¯ æ ¸å¿ƒèƒ½åŠ›:
1. æ·±åº¦åˆ†æå’Œç†è§£ç”¨æˆ·é—®é¢˜
2. å¿…é¡»ä½¿ç”¨å·¥å…·è§£å†³é—®é¢˜(å±•ç¤ºæ¨ç†èƒ½åŠ›)
3. è‡ªä¸»å†³å®šå·¥å…·å‚æ•°(å±•ç¤ºå†³ç­–èƒ½åŠ›)
4. åŸºäºç»“æœè¿›è¡Œç»¼åˆæ¨ç†

ğŸ› ï¸ å¯ç”¨å·¥å…·:
{tools_text}

âš ï¸ é‡è¦è§„åˆ™(å¿…é¡»ä¸¥æ ¼éµå®ˆ):
1. **æ•°å­¦è®¡ç®—å¿…é¡»è°ƒç”¨calculator** - å³ä½¿ç®€å•å¦‚"1+1"
2. **æ—¶é—´æŸ¥è¯¢å¿…é¡»è°ƒç”¨time_tool** - ä¸è¦çŒœæµ‹
3. **æ–‡æœ¬ç»Ÿè®¡å¿…é¡»è°ƒç”¨text_analyzer** - ä¸è¦ä¼°ç®—
4. **å¯¹è¯ç»“æŸå¿…é¡»è°ƒç”¨end_conversation_detector** - æ£€æµ‹åˆ°"å†è§"ç­‰å…³é”®è¯æ—¶å¼ºåˆ¶è°ƒç”¨

ğŸ”„ æ¨ç†æµç¨‹:
ç¬¬1æ­¥: åˆ†æç”¨æˆ·é—®é¢˜ç±»å‹å’Œæ„å›¾
ç¬¬2æ­¥: ç¡®å®šéœ€è¦ä½¿ç”¨çš„å·¥å…·(æ ¹æ®ä¸Šè¿°è§„åˆ™)
ç¬¬3æ­¥: è‡ªä¸»å†³å®šå·¥å…·çš„è¾“å…¥å‚æ•°
ç¬¬4æ­¥: ç­‰å¾…å·¥å…·æ‰§è¡Œç»“æœ
ç¬¬5æ­¥: åŸºäºç»“æœè¿›è¡Œæ¨ç†å¹¶ç”Ÿæˆç­”æ¡ˆ

ç°åœ¨å¼€å§‹ä¸¥æ ¼éµå®ˆè§„åˆ™,å¸®åŠ©ç”¨æˆ·!"""

    def _convert_tools_to_openai_format(self) -> List[Dict]:
        """
        å°†LangChainå·¥å…·è½¬æ¢ä¸ºOpenAI Function Callingæ ¼å¼

        è¿™æ˜¯æ··åˆæ¶æ„çš„å…³é”®: ä¿ç•™LangChainå·¥å…·å®šä¹‰,ä½†ç”¨OpenAIæ ¼å¼è°ƒç”¨
        """
        openai_tools = []

        for tool in self.tools:
            # æå–å‚æ•°schema
            if hasattr(tool, 'args_schema') and tool.args_schema:
                # ä½¿ç”¨model_json_schemaæ›¿ä»£deprecatedçš„schemaæ–¹æ³•
                parameters = tool.args_schema.model_json_schema()
            else:
                parameters = {
                    "type": "object",
                    "properties": {},
                    "required": []
                }

            # è½¬æ¢ä¸ºOpenAIæ ¼å¼
            openai_tool = {
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": parameters
                }
            }

            openai_tools.append(openai_tool)

        return openai_tools

    def _execute_tool(self, tool_name: str, arguments: Dict) -> str:
        """
        æ‰§è¡ŒLangChainå·¥å…·

        Args:
            tool_name: å·¥å…·åç§°
            arguments: å·¥å…·å‚æ•°

        Returns:
            å·¥å…·æ‰§è¡Œç»“æœ
        """
        if tool_name not in self.tool_map:
            return f"é”™è¯¯: å·¥å…· '{tool_name}' ä¸å­˜åœ¨"

        try:
            tool = self.tool_map[tool_name]
            result = tool._run(**arguments)
            return str(result)
        except Exception as e:
            return f"å·¥å…·æ‰§è¡Œé”™è¯¯: {str(e)}"

    def _check_end_keywords(self, user_input: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«ç»“æŸå…³é”®è¯"""
        end_keywords = [
            'å†è§', 'æ‹œæ‹œ', 'bye', 'goodbye', 'é€€å‡º', 'ç»“æŸ',
            'å…³é—­', 'ç¦»å¼€', 'ä¸èŠäº†', 'èµ°äº†', 'quit', 'exit',
            '886', '88', 'ä¸‹çº¿', 'æ–­å¼€'
        ]
        user_lower = user_input.lower().strip()
        return any(keyword in user_lower for keyword in end_keywords)

    def run(
        self,
        user_input: str,
        show_reasoning: bool = True
    ) -> AgentResponse:
        """
        æ‰§è¡Œæ¨ç†(éæµå¼)

        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            show_reasoning: æ˜¯å¦æ˜¾ç¤ºæ¨ç†è¿‡ç¨‹

        Returns:
            AgentResponse: æ‰§è¡Œç»“æœ
        """
        if show_reasoning:
            print("\n" + "="*70)
            print("ğŸ§  æ··åˆæ¶æ„æ¨ç†è¿‡ç¨‹(OpenAIåŸç”Ÿ + LangChainå·¥å…·)")
            print("="*70)

        # æ£€æµ‹ç»“æŸå…³é”®è¯
        contains_end_keyword = self._check_end_keywords(user_input)
        if contains_end_keyword and show_reasoning:
            print(f"\nğŸ” é¢„å¤„ç†: æ£€æµ‹åˆ°ç»“æŸå…³é”®è¯,å°†å¼ºåˆ¶è¦æ±‚è°ƒç”¨end_conversation_detector")

        # æ„å»ºæ¶ˆæ¯(åˆ©ç”¨KV Cache)
        messages = self._build_messages(user_input, contains_end_keyword)

        # æ¨ç†æ­¥éª¤è®°å½•
        reasoning_steps = []
        tool_call_count = 0

        try:
            # ç¬¬ä¸€æ¬¡è°ƒç”¨: æ¨¡å‹å†³ç­–
            if show_reasoning:
                print(f"\n{'â”€'*70}")
                print("ğŸ“¡ è°ƒç”¨OpenAI APIè¿›è¡Œæ¨ç†...")
                print(f"{'â”€'*70}")

            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                tools=self.openai_tools,
                tool_choice="auto",
                temperature=self.temperature
            )

            assistant_message = response.choices[0].message

            # å¤„ç†å·¥å…·è°ƒç”¨
            if assistant_message.tool_calls:
                if show_reasoning:
                    print(f"\nâœ… æ¨¡å‹å†³å®šè°ƒç”¨å·¥å…·(å…±{len(assistant_message.tool_calls)}ä¸ª)")

                # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°å†å²
                messages.append({
                    "role": "assistant",
                    "content": assistant_message.content or "",
                    "tool_calls": [
                        {
                            "id": tc.id,
                            "type": "function",
                            "function": {
                                "name": tc.function.name,
                                "arguments": tc.function.arguments
                            }
                        } for tc in assistant_message.tool_calls
                    ]
                })

                # æ‰§è¡Œæ¯ä¸ªå·¥å…·è°ƒç”¨
                for tool_call in assistant_message.tool_calls:
                    tool_call_count += 1
                    tool_name = tool_call.function.name
                    arguments = json.loads(tool_call.function.arguments)

                    if show_reasoning:
                        self._display_tool_call(
                            tool_call_count,
                            tool_name,
                            arguments
                        )

                    # æ‰§è¡ŒLangChainå·¥å…·
                    result = self._execute_tool(tool_name, arguments)

                    if show_reasoning:
                        self._display_tool_result(result)

                    # è®°å½•æ¨ç†æ­¥éª¤
                    reasoning_steps.append({
                        'step': tool_call_count,
                        'tool': tool_name,
                        'arguments': arguments,
                        'result': result
                    })

                    # æ·»åŠ å·¥å…·ç»“æœåˆ°æ¶ˆæ¯
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "content": result
                    })

                # ç¬¬äºŒæ¬¡è°ƒç”¨: åŸºäºå·¥å…·ç»“æœç”Ÿæˆæœ€ç»ˆå›ç­”
                if show_reasoning:
                    print(f"\n{'â”€'*70}")
                    print("ğŸ’­ æ¨¡å‹åŸºäºå·¥å…·ç»“æœç”Ÿæˆæœ€ç»ˆå›ç­”...")
                    print(f"{'â”€'*70}")

                final_response = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    temperature=self.temperature
                )

                final_answer = final_response.choices[0].message.content
            else:
                # æ²¡æœ‰å·¥å…·è°ƒç”¨,ç›´æ¥å›ç­”
                if show_reasoning:
                    print("\nâš ï¸  æ¨¡å‹é€‰æ‹©ç›´æ¥å›ç­”(æœªè°ƒç”¨å·¥å…·)")
                final_answer = assistant_message.content

            # æ›´æ–°å¯¹è¯å†å²(ç”¨äºKV Cache)
            if self.enable_cache:
                self.conversation_history.append({
                    "role": "user",
                    "content": user_input
                })
                self.conversation_history.append({
                    "role": "assistant",
                    "content": final_answer
                })

            if show_reasoning:
                print(f"\n{'='*70}")
                print("ğŸ’¬ æœ€ç»ˆå›ç­”")
                print(f"{'='*70}")
                print(final_answer)
                print(f"{'='*70}\n")

            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç»“æŸå¯¹è¯
            should_end = any(
                step['tool'] == 'end_conversation_detector' and
                'END_CONVERSATION' in step['result']
                for step in reasoning_steps
            )

            return AgentResponse(
                success=True,
                output=final_answer,
                reasoning_steps=reasoning_steps,
                tool_calls=tool_call_count,
                metadata={
                    'should_end': should_end,
                    'cached_tokens': len(self.conversation_history) if self.enable_cache else 0
                }
            )

        except Exception as e:
            error_msg = f"æ‰§è¡Œé”™è¯¯: {str(e)}"
            print(f"\nâŒ {error_msg}")
            return AgentResponse(
                success=False,
                output=error_msg,
                error=str(e)
            )

    def _build_messages(self, user_input: str, force_end_detection: bool = False) -> List[Dict]:
        """
        æ„å»ºæ¶ˆæ¯åˆ—è¡¨

        ç³»ç»Ÿæç¤ºè¯ä¼šè¢«OpenAIè‡ªåŠ¨ç¼“å­˜(Prompt Caching)
        å¯¹è¯å†å²ä¹Ÿä¼šè¢«KV Cacheä¼˜åŒ–
        """
        messages = [
            {"role": "system", "content": self.system_prompt}
        ]

        # æ·»åŠ å¯¹è¯å†å²(KV Cacheä¼˜åŒ–)
        if self.enable_cache:
            messages.extend(self.conversation_history)

        # æ·»åŠ å½“å‰è¾“å…¥
        user_message = user_input
        if force_end_detection:
            user_message += "\n\n[ç³»ç»Ÿè¦æ±‚: æ£€æµ‹åˆ°ç»“æŸå…³é”®è¯,å¿…é¡»è°ƒç”¨end_conversation_detectorå·¥å…·]"

        messages.append({
            "role": "user",
            "content": user_message
        })

        return messages

    def _display_tool_call(self, step: int, tool_name: str, arguments: Dict):
        """æ˜¾ç¤ºå·¥å…·è°ƒç”¨ä¿¡æ¯"""
        print(f"\n{'='*70}")
        print(f"ğŸ“ æ¨ç†æ­¥éª¤ {step}")
        print(f"{'='*70}")
        print(f"\nâœ… æ¨¡å‹å†³ç­–:")
        print(f"   â†’ é€‰æ‹©å·¥å…·: {tool_name}")
        print(f"\nğŸ“¥ æ¨¡å‹å†³å®šçš„å‚æ•°:")
        print(f"{'â”€'*70}")
        formatted_args = json.dumps(arguments, ensure_ascii=False, indent=6)
        for line in formatted_args.split('\n'):
            print(f"   {line}")
        print(f"{'â”€'*70}")

    def _display_tool_result(self, result: str):
        """æ˜¾ç¤ºå·¥å…·æ‰§è¡Œç»“æœ"""
        print(f"\nğŸ“¤ å·¥å…·è¿”å›ç»“æœ:")
        print(f"{'â”€'*70}")
        if len(result) > 500:
            print(f"   {result[:500]}...")
            print(f"   ... (ç»“æœè¿‡é•¿,å·²æˆªæ–­)")
        else:
            for line in result.split('\n'):
                print(f"   {line}")
        print(f"{'â”€'*70}")

    def clear_history(self):
        """æ¸…é™¤å¯¹è¯å†å²ç¼“å­˜"""
        self.conversation_history = []
        print("âœ… å¯¹è¯å†å²å·²æ¸…é™¤(KV Cacheé‡ç½®)")

    def get_stats(self) -> Dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        base_stats = super().get_stats()
        return {
            **base_stats,
            'estimated_cached_tokens': sum(
                len(msg['content']) // 4
                for msg in self.conversation_history
            ),
            'system_prompt_tokens': len(self.system_prompt) // 4,
            'tools_count': len(self.tools)
        }


# å¯¼å‡º
__all__ = ['HybridReasoningAgent']
