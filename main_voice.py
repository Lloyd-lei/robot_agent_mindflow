#!/usr/bin/env python3
"""
è¯­éŸ³äº¤äº’ Agent ä¸»ç¨‹åº
å®Œæ•´çš„è¯­éŸ³äº¤äº’é—­ç¯ï¼šå½•éŸ³ â†’ VAD â†’ ASR â†’ LLM â†’ TTS â†’ æ’­æ”¾
"""
import sys
from pathlib import Path
import pyaudio
import numpy as np
import wave
import time
from colorama import Fore, Style, init

from conversation_session import ConversationSession
from asr_interface import OpenAIASR
from voice_feedback import VoiceWaitingFeedback
import config

init(autoreset=True)


class SimpleVAD:
    """ç®€åŒ–çš„ VADï¼ˆåŸºäºèƒ½é‡æ£€æµ‹ï¼‰"""
    
    def __init__(self, energy_threshold: float = 1100.0, silence_duration_ms: int = 900, frame_duration_ms: int = 30):
        self.energy_threshold = energy_threshold
        self.silence_duration_ms = silence_duration_ms
        self.num_silence_frames = int(silence_duration_ms / frame_duration_ms)
    
    def is_speech(self, audio_data: bytes) -> tuple[bool, float]:
        """åˆ¤æ–­æ˜¯å¦ä¸ºè¯­éŸ³"""
        samples = np.frombuffer(audio_data, dtype=np.int16)
        energy = np.sqrt(np.mean(samples.astype(np.float32) ** 2))
        return energy > self.energy_threshold, energy


class VoiceRecorder:
    """è¯­éŸ³å½•éŸ³å™¨ï¼ˆå¸¦ VADï¼‰"""
    
    def __init__(self, vad: SimpleVAD, sample_rate: int = 16000, chunk_size: int = 480):
        self.vad = vad
        self.sample_rate = sample_rate
        self.chunk_size = chunk_size
        self.audio = pyaudio.PyAudio()
        self.stream = None
    
    def record_until_silence(self, max_duration: float = 30.0) -> tuple[bytes, dict]:
        """å½•éŸ³ç›´åˆ°æ£€æµ‹åˆ°é™éŸ³"""
        print(f"{Fore.YELLOW}ğŸ¤ æ­£åœ¨ç›‘å¬...ï¼ˆè¯·è¯´è¯ï¼‰\n")
        
        try:
            self.stream = self.audio.open(
                format=pyaudio.paInt16,
                channels=1,
                rate=self.sample_rate,
                input=True,
                frames_per_buffer=self.chunk_size
            )
        except Exception as e:
            print(f"{Fore.RED}âŒ æ— æ³•æ‰“å¼€éŸ³é¢‘æµ: {e}")
            return b'', {}
        
        audio_frames = []
        silence_frames = 0
        speech_started = False
        start_time = time.time()
        max_energy = 0
        energies = []
        
        try:
            while True:
                if time.time() - start_time > max_duration:
                    break
                
                try:
                    frame = self.stream.read(self.chunk_size, exception_on_overflow=False)
                except Exception as e:
                    print(f"\n{Fore.RED}âŒ è¯»å–éŸ³é¢‘å¤±è´¥: {e}")
                    break
                
                is_speech, energy = self.vad.is_speech(frame)
                energies.append(energy)
                max_energy = max(max_energy, energy)
                
                # æ˜¾ç¤ºèƒ½é‡æ¡
                bar_length = int(min(energy / 50, 40))
                bar = "â–ˆ" * bar_length
                status = "ğŸ—£ï¸" if is_speech else "ğŸ”‡"
                print(f"\r{Fore.GREEN if is_speech else Fore.YELLOW}èƒ½é‡: {energy:7.1f} |{bar:<40}| {status}", end='', flush=True)
                
                if not speech_started:
                    if is_speech:
                        speech_started = True
                        audio_frames.append(frame)
                        silence_frames = 0
                        print(f"\n{Fore.GREEN}âœ… å¼€å§‹å½•éŸ³...\n")
                else:
                    audio_frames.append(frame)
                    if is_speech:
                        silence_frames = 0
                    else:
                        silence_frames += 1
                        if silence_frames >= self.vad.num_silence_frames:
                            print(f"\n{Fore.GREEN}âœ… æ£€æµ‹åˆ°é™éŸ³ï¼Œåœæ­¢å½•éŸ³\n")
                            break
        
        except KeyboardInterrupt:
            print(f"\n{Fore.YELLOW}âš ï¸  ç”¨æˆ·ä¸­æ–­")
        
        finally:
            if self.stream:
                self.stream.stop_stream()
                self.stream.close()
        
        audio_data = b''.join(audio_frames)
        duration = len(audio_data) / (self.sample_rate * 2)
        
        stats = {
            'duration': duration,
            'frames': len(audio_frames),
            'max_energy': max_energy,
            'avg_energy': np.mean(energies) if energies else 0,
            'speech_started': speech_started
        }
        
        return audio_data, stats
    
    def save_wav(self, audio_data: bytes, filename: str):
        """ä¿å­˜ä¸º WAV æ–‡ä»¶"""
        with wave.open(filename, 'wb') as wf:
            wf.setnchannels(1)
            wf.setsampwidth(2)
            wf.setframerate(self.sample_rate)
            wf.writeframes(audio_data)
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.stream:
            self.stream.close()
        if self.audio:
            self.audio.terminate()


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 80)
    print(Fore.CYAN + Style.BRIGHT + "ğŸ¤ è¯­éŸ³äº¤äº’ AI Agent")
    print("=" * 80 + "\n")
    
    # è¯¢é—®ç”¨æˆ·æ˜¯å¦æˆ´è€³æœº
    print(f"{Fore.YELLOW}è¯·é€‰æ‹©éº¦å…‹é£ç±»å‹ï¼š")
    print(f"  1. AirPods Pro / è“ç‰™è€³æœº (æ¨èé˜ˆå€¼: 1100)")
    print(f"  2. MacBook Pro å†…ç½®éº¦å…‹é£ (æ¨èé˜ˆå€¼: 300)")
    
    choice = input(f"\n{Fore.YELLOW}è¯·è¾“å…¥é€‰é¡¹ (1/2ï¼Œé»˜è®¤1): {Style.RESET_ALL}").strip()
    
    if choice == "2":
        vad_threshold = 300.0
        mic_type = "MacBook Pro å†…ç½®éº¦å…‹é£"
    else:
        vad_threshold = 1100.0
        mic_type = "AirPods Pro / è“ç‰™è€³æœº"
    
    print(f"\n{Fore.GREEN}âœ… å·²é€‰æ‹©: {mic_type}\n")
    
    # åˆå§‹åŒ– VAD
    vad = SimpleVAD(energy_threshold=vad_threshold, silence_duration_ms=700)
    print(f"{Fore.GREEN}âœ… VAD åˆå§‹åŒ–: é˜ˆå€¼={vad_threshold}, é™éŸ³=900ms\n")
    
    # åˆå§‹åŒ–å½•éŸ³å™¨
    recorder = VoiceRecorder(vad)
    
    # åˆå§‹åŒ– ASR
    print(f"{Fore.CYAN}â³ åˆå§‹åŒ– ASR...")
    asr = OpenAIASR(api_key=config.OPENAI_API_KEY, model="whisper-1")
    print(f"{Fore.GREEN}âœ… ASR åˆå§‹åŒ–å®Œæˆ\n")
    
    # åˆå§‹åŒ– ConversationSessionï¼ˆLLM + TTSï¼‰
    print(f"{Fore.CYAN}â³ åˆå§‹åŒ– LLM + TTS...")
    session = ConversationSession(
        llm_model=config.LLM_MODEL,
        tts_provider="openai",
        tts_voice="shimmer",
        enable_cache=True,
        show_reasoning=True,
        timeout=60,
        tts_wait_timeout=60,
        voice_mode=True  # âœ… å¯ç”¨è¯­éŸ³æ¨¡å¼ï¼ˆå·¥å…·è°ƒç”¨éŸ³æ•ˆï¼‰
    )
    session.start()
    
    print(f"{Fore.GREEN}âœ… LLM + TTS åˆå§‹åŒ–å®Œæˆ")
    print(f"{Fore.GREEN}âœ… éŸ³æ•ˆç³»ç»Ÿå·²å¯ç”¨\n")
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•
    temp_dir = Path("temp_audio")
    temp_dir.mkdir(exist_ok=True)
    
    round_num = 0
    
    try:
        while True:
            round_num += 1
            print(f"\n{Fore.MAGENTA}{'='*80}")
            print(f"{Fore.MAGENTA}ğŸ”„ å¯¹è¯è½®æ¬¡ {round_num}")
            print(f"{Fore.MAGENTA}{'='*80}\n")
            
            # 1. å½•éŸ³ï¼ˆå¸¦ VADï¼‰
            audio_data, stats = recorder.record_until_silence(max_duration=30.0)
            
            if not audio_data or not stats.get('speech_started'):
                print(f"{Fore.RED}âš ï¸  æœªæ£€æµ‹åˆ°æœ‰æ•ˆè¯­éŸ³\n")
                continue
            
            # 2. ä¿å­˜éŸ³é¢‘
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            audio_file = temp_dir / f"recording_{timestamp}.wav"
            recorder.save_wav(audio_data, str(audio_file))
            print(f"{Fore.GREEN}ğŸ’¾ å·²ä¿å­˜: {audio_file}\n")
            
            # 3. ASR è¯†åˆ«
            print(f"{Fore.CYAN}ğŸ”„ æ­£åœ¨è¯†åˆ«...\n")
            asr_start = time.time()
            result = asr.transcribe(str(audio_file))
            asr_time = time.time() - asr_start
            
            if not result.text:
                print(f"{Fore.RED}âŒ ASR è¯†åˆ«å¤±è´¥\n")
                continue
            
            print(f"{Fore.GREEN}ğŸ“ è¯†åˆ«ç»“æœ: {Fore.WHITE}{result.text}")
            print(f"{Fore.GREEN}ğŸŒ è¯­è¨€: {Fore.WHITE}{result.language}\n")
            
            # 4. LLM æ¨ç† + TTS æ’­æ”¾
            print(f"{Fore.CYAN}ğŸ§  LLM æ¨ç†ä¸­...\n")
            
            session_result = session.chat(result.text)
            
            if session_result.should_end:
                print(f"\n{Fore.YELLOW}ğŸ‘‹ æ£€æµ‹åˆ°å¯¹è¯ç»“æŸï¼Œå†è§ï¼\n")
                break
            
            print(f"\n{Fore.GREEN}âœ… å¯¹è¯è½®æ¬¡ {round_num} å®Œæˆ\n")
    
    except KeyboardInterrupt:
        print(f"\n\n{Fore.YELLOW}âš ï¸  ç”¨æˆ·ä¸­æ–­ï¼Œå†è§ï¼\n")
    
    finally:
        # æ¸…ç†èµ„æº
        print(f"{Fore.CYAN}ğŸ§¹ æ¸…ç†èµ„æº...\n")
        recorder.cleanup()
        session.end()
        print(f"{Fore.GREEN}âœ… ç¨‹åºç»“æŸ\n")


if __name__ == "__main__":
    main()
