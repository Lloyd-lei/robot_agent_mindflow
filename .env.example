# ============== LLM 提供商选择 ==============
# true = 使用 Ollama 本地模型（免费、隐私、快速）
# false = 使用 OpenAI 云端模型（需要付费API）
USE_OLLAMA=true

# ============== Ollama 配置（本地模型）==============
# 推荐模型：
#   qwen2.5:1.5b  - 1GB，超快，适合测试
#   qwen2.5:7b    - 4.7GB，推荐，性能均衡
#   qwen2.5:14b   - 8.7GB，最强，质量最高
#   llama3.2:3b   - 2GB，快速
#   llama3.1:8b   - 4.7GB，通用
OLLAMA_MODEL=qwen2.5:3b
OLLAMA_BASE_URL=http://localhost:11434/v1

# ============== OpenAI 配置（云端API）==============
# 如果 USE_OLLAMA=false，需要填写以下信息
OPENAI_API_KEY=sk-your-api-key-here
OPENAI_MODEL=gpt-4o-mini

# ============== 通用配置 ==============
TEMPERATURE=0

# ============== 使用说明 ==============
# 1. 复制此文件为 .env
# 2. 如果使用 Ollama：
#    - 安装 Ollama: brew install ollama (macOS)
#    - 启动服务: ollama serve
#    - 下载模型: ollama pull qwen2.5:7b
#    - 设置 USE_OLLAMA=true
# 3. 如果使用 OpenAI：
#    - 设置 USE_OLLAMA=false
#    - 填写你的 OPENAI_API_KEY
