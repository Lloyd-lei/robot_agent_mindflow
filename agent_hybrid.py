"""
Agent - OpenAIåŸç”ŸFunction Calling + LangChainå·¥å…·æ±  + KV Cacheä¼˜åŒ–

æ²¡ç”¨openaiçš„æ—¶å€™ï¼Œå¯ä»¥åˆ‡æ¢åˆ°ollamaï¼Œollamaæ²¡æœ‰tool_choiceï¼Œä½†èƒ½ç”¨longchain

æ ¸å¿ƒç‰¹æ€§ï¼š
1. ä½¿ç”¨OpenAIåŸç”ŸAPIè·å¾—å®Œå…¨æ§åˆ¶æƒï¼ˆtool_choiceæ§åˆ¶ï¼‰
2. ä¿ç•™LangChainä¸°å¯Œçš„å·¥å…·ç”Ÿæ€
3. KV Cacheè‡ªåŠ¨ä¼˜åŒ–ï¼ˆç³»ç»Ÿæç¤ºè¯ç¼“å­˜ã€å¯¹è¯å†å²ç¼“å­˜ï¼‰
4. 100%å¯é çš„å·¥å…·è°ƒç”¨
5. å®Œæ•´çš„æ¨ç†è¿‡ç¨‹å±•ç¤º
"""

from openai import OpenAI
from typing import List, Dict, Any, Generator, Optional
import json
import re
from datetime import datetime

# å¯¼å…¥æ—¥å¿—ç³»ç»Ÿ
from logger_config import get_logger

# å¯¼å…¥LangChainå·¥å…·
from tools import (
    CalculatorTool,
    TimeTool,
    TextAnalysisTool,
    UnitConversionTool,
    ComparisonTool,
    LogicReasoningTool,
    LibraryManagementTool,
    ConversationEndDetector,
    WebSearchTool,
    FileOperationTool,
    ReminderTool,
    VisitorRegistrationTool,
    MeetingRoomTool,
    EmployeeDirectoryTool,
    DirectionGuideTool,
    PackageManagementTool,
    FAQTool
) #çœŸä»–å¦ˆé•¿
import config

# å¯¼å…¥TTSä¼˜åŒ–å’Œè¯­éŸ³åé¦ˆ
from tts_optimizer import TTSOptimizer # ttsoptimizeræ²¡æœ‰å®ç°streaming piplineï¼Œæ‰€ä»¥å¥½åƒæ²¡ç”¨ä¸Š
from voice_feedback import VoiceWaitingFeedback # responseç©ºçª—æœŸæ’­æ”¾å£°éŸ³ï¼Œé˜²æ­¢ç”¨æˆ·ç­‰å¾…ç„¦è™‘ï¼Œä½†è¿˜æ²¡æ‰¾åˆ°åˆé€‚çš„éŸ³æ•ˆ
from tts_interface import TTSFactory, TTSProvider # ttsinterfaceæ˜¯ttsçš„èŒƒå‹æ¥å£ï¼Œå¯ä»¥è½»æ¾åˆ‡æ¢ttsæœåŠ¡ï¼Œç°åœ¨æ˜¯edge ttsï¼Œå¯ä»¥æ¢openaiæˆ–è€…è‡ªå·±çš„tts
from streaming_tts_pipeline import StreamingTTSPipeline, create_streaming_pipeline # streaming_tts_pipelineæ˜¯æµå¼ttsçš„å®ç°


class HybridReasoningAgent:
    """
    æ··åˆæ¶æ„æ¨ç†Agent
    
    æ¶æ„ï¼š
    - OpenAIåŸç”ŸAPIï¼šæ¨ç†å¼•æ“ï¼ˆå¯æ§ã€å¿«é€Ÿã€æ”¯æŒKV Cacheï¼‰
    - LangChainå·¥å…·ï¼šæ‰§è¡Œå¼•æ“ï¼ˆä¸°å¯Œã€æ˜“æ‰©å±•ï¼‰
    - KV Cacheï¼šæ€§èƒ½ä¼˜åŒ–ï¼ˆå¯¹è¯å†å²ã€ç³»ç»Ÿæç¤ºè¯è‡ªåŠ¨ç¼“å­˜ï¼‰
    """
    
    def __init__(
        self,
        api_key: str = None,
        model: str = None,
        temperature: float = None,
        enable_cache: bool = True,
        enable_tts: bool = False,
        voice_mode: bool = False,
        tts_engine: Optional[callable] = None,
        enable_streaming_tts: bool = False
    ):
        """
        åˆå§‹åŒ–æ··åˆæ¶æ„Agent
        
        Args:
            api_key: OpenAI APIå¯†é’¥
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            enable_cache: æ˜¯å¦å¯ç”¨å¯¹è¯å†å²ç¼“å­˜ï¼ˆKV Cacheä¼˜åŒ–ï¼‰
            enable_tts: æ˜¯å¦å¯ç”¨TTSä¼˜åŒ–ï¼ˆä¼ ç»Ÿæ‰¹é‡æ¨¡å¼ï¼‰
            voice_mode: æ˜¯å¦å¯ç”¨è¯­éŸ³ç­‰å¾…åé¦ˆ
            tts_engine: TTSå¼•æ“å‡½æ•°ï¼ˆå¯é€‰ï¼‰
            enable_streaming_tts: æ˜¯å¦å¯ç”¨æµå¼TTSï¼ˆæ¨èï¼Œæ›´ä½å»¶è¿Ÿï¼‰
        """
        self.api_key = api_key or config.LLM_API_KEY
        self.model = model or config.LLM_MODEL
        self.temperature = temperature if temperature is not None else config.TEMPERATURE
        self.enable_cache = enable_cache
        self.enable_tts = enable_tts
        self.voice_mode = voice_mode
        self.enable_streaming_tts = enable_streaming_tts
        
        # æ—¥å¿—ç³»ç»Ÿ
        self.logger = get_logger(self.__class__.__name__)
        
        # OpenAIå®¢æˆ·ç«¯ï¼ˆå…¼å®¹Ollamaï¼‰
        if config.LLM_BASE_URL:
            # ä½¿ç”¨è‡ªå®šä¹‰base_urlï¼ˆOllamaæˆ–å…¶ä»–å…¼å®¹æœåŠ¡ï¼‰
            self.client = OpenAI(api_key=self.api_key, base_url=config.LLM_BASE_URL)
        else:
            # ä½¿ç”¨OpenAIå®˜æ–¹æœåŠ¡
            self.client = OpenAI(api_key=self.api_key)
        
        # LangChainå·¥å…·æ± 
        self.langchain_tools = self._init_langchain_tools()
        
        # è½¬æ¢ä¸ºOpenAIæ ¼å¼
        self.openai_tools = self._convert_tools_to_openai_format()
        
        # å·¥å…·åç§°æ˜ å°„
        self.tool_map = {tool.name: tool for tool in self.langchain_tools}
        
        # å¯¹è¯å†å²ï¼ˆKV Cacheä¼šè‡ªåŠ¨ç¼“å­˜ï¼‰
        self.conversation_history = []
        
        # ç³»ç»Ÿæç¤ºè¯ï¼ˆä¼šè¢«KV Cacheç¼“å­˜ï¼ŒèŠ‚çœæˆæœ¬ï¼‰
        self.system_prompt = self._create_system_prompt()
        
        # TTSå¼•æ“ï¼ˆå…±äº«ï¼‰
        self.tts_engine = tts_engine
        if (self.enable_tts or self.enable_streaming_tts) and tts_engine is None:
            print(f"ğŸµ ä½¿ç”¨ Edge TTSï¼ˆæ™“æ™“è¯­éŸ³ï¼Œè¯­é€Ÿ +15%ï¼‰...")
            self.tts_engine = TTSFactory.create_tts(
                provider=TTSProvider.EDGE,
                voice="zh-CN-XiaoxiaoNeural",  # æ™“æ™“ - æ¸©æŸ”å¥³å£° # è¿˜æœ‰åˆ«çš„å£°éŸ³ï¼Œå¯ä»¥æ¢
                rate="+15%",    # ğŸ”§ è¯­é€ŸåŠ å¿« 15%ï¼ˆæ›´è‡ªç„¶ï¼‰
                volume="+10%"   # ğŸ”§ éŸ³é‡ç¨å¤§
            )
        
        # TTSä¼˜åŒ–å™¨ï¼ˆä¼ ç»Ÿæ‰¹é‡æ¨¡å¼ï¼‰
        if self.enable_tts:
            self.tts_optimizer = TTSOptimizer(
                tts_engine=self.tts_engine,
                max_chunk_length=100,
                max_retries=3,
                timeout_per_chunk=10,
                buffer_size=3
            )
        
        # æµå¼TTSç®¡é“ï¼ˆæ¨èæ¨¡å¼ï¼‰
        """
        æœ€å¤§ç¨‹åº¦ä¿è¯æ²¡æœ‰èƒŒå‹
        
        æ ¹æ®å†…å­˜å’Œå®‰å…¨æ€§å¤§å°ï¼Œå¯ä»¥é‡æ–°é…ç½®

        å‚æ•°è¯´æ˜ï¼š
        - text_queue_size: æ–‡æœ¬é˜Ÿåˆ—å¤§å°
        - audio_queue_size: éŸ³é¢‘é˜Ÿåˆ—å¤§å°
        - max_tasks: æœ€å¤§ä»»åŠ¡æ•°
        - generation_timeout: ç”Ÿæˆè¶…æ—¶æ—¶é—´
        - playback_timeout: æ’­æ”¾è¶…æ—¶æ—¶é—´
        - min_chunk_length: æœ€å°å¥å­é•¿åº¦
        - max_chunk_length: æœ€å¤§å¥å­é•¿åº¦
        """
        self.streaming_pipeline = None
        if self.enable_streaming_tts:
            self.streaming_pipeline = create_streaming_pipeline(
                tts_engine=self.tts_engine,
                text_queue_size=15,
                audio_queue_size=10,
                max_tasks=50,
                generation_timeout=15.0,
                playback_timeout=30.0,
                min_chunk_length=3, # æœ€å°å¥å­é•¿åº¦
                max_chunk_length=150, # æœ€å¤§å¥å­é•¿åº¦
                verbose=True
            )
            print(f"æµå¼TTSç®¡é“å·²åˆ›å»º")
        
        # è¯­éŸ³åé¦ˆ
        if self.voice_mode:
            self.voice_feedback = VoiceWaitingFeedback(mode='text')
        
        print(f"   æ··åˆæ¶æ„Agentåˆå§‹åŒ–æˆåŠŸ")
        print(f"   å¼•æ“: OpenAIåŸç”ŸAPI ({self.model})")
        print(f"   å·¥å…·: LangChainå·¥å…·æ±  ({len(self.langchain_tools)}ä¸ª)")
        print(f"   KV Cache: {'å¯ç”¨' if self.enable_cache else 'ç¦ç”¨'}")
        print(f"   TTSä¼˜åŒ–: {'å¯ç”¨' if self.enable_tts else 'ç¦ç”¨'}")
        print(f"   æµå¼TTS: {'å¯ç”¨ âš¡' if self.enable_streaming_tts else 'ç¦ç”¨'}")
        print(f"   è¯­éŸ³æ¨¡å¼: {'å¯ç”¨' if self.voice_mode else 'ç¦ç”¨'}")
        print(f"   æ¸©åº¦: {self.temperature}")
        print()
    
    def _init_langchain_tools(self) -> List:
        """åˆå§‹åŒ–LangChainå·¥å…·æ± """
        return [
            # åŸºç¡€å·¥å…·
            CalculatorTool(),
            TimeTool(),
            TextAnalysisTool(),
            UnitConversionTool(),
            ComparisonTool(),
            LogicReasoningTool(),
            LibraryManagementTool(),
            ConversationEndDetector(),
            WebSearchTool(),
            FileOperationTool(),
            ReminderTool(),
            # å‰å°æ¥å¾…å·¥å…·
            VisitorRegistrationTool(),
            MeetingRoomTool(),
            EmployeeDirectoryTool(),
            DirectionGuideTool(),
            PackageManagementTool(),
            FAQTool(),
        ]
    
    def _create_system_prompt(self) -> str:
        """
        åˆ›å»ºç³»ç»Ÿæç¤ºè¯
        æ³¨æ„ï¼šè¿™ä¸ªæç¤ºè¯ä¼šè¢«OpenAIè‡ªåŠ¨ç¼“å­˜ï¼ˆPrompt Cachingï¼‰ï¼ŒèŠ‚çœ50%æˆæœ¬
        """
        return """ä½ æ˜¯ä¸€ä¸ªå…·æœ‰å¼ºå¤§æ¨ç†èƒ½åŠ›çš„AIè¯­éŸ³åŠ©æ‰‹ã€‚ä½ å«èŒ¶èŒ¶ã€‚ä½ çš„å›ç­”å¿…é¡»ä¸ºttsä¼˜åŒ–ï¼Œä¸èƒ½å‡ºç°è¡¨æƒ…åŒ…å’Œç‰¹æ®Šç¬¦å·ã€‚

æ ¸å¿ƒèƒ½åŠ›ï¼š
1. æ·±åº¦åˆ†æå’Œç†è§£ç”¨æˆ·é—®é¢˜
2. å¿…é¡»ä½¿ç”¨å·¥å…·è§£å†³é—®é¢˜ï¼ˆå±•ç¤ºæ¨ç†èƒ½åŠ›ï¼‰
3. è‡ªä¸»å†³å®šå·¥å…·å‚æ•°ï¼ˆå±•ç¤ºå†³ç­–èƒ½åŠ›ï¼‰
4. åŸºäºç»“æœè¿›è¡Œç»¼åˆæ¨ç†

å¯ç”¨å·¥å…·ï¼š
- calculator: æ•°å­¦è®¡ç®—ï¼ˆsqrtã€ä¸‰è§’å‡½æ•°ã€å¤æ‚è¿ç®—ï¼‰
- time_tool: æ—¶é—´æŸ¥è¯¢ï¼ˆå½“å‰æ—¶é—´ã€æ—¥æœŸã€æ˜ŸæœŸï¼‰
- text_analyzer: æ–‡æœ¬åˆ†æï¼ˆå­—æ•°ç»Ÿè®¡ã€å¥å­åˆ†æï¼‰
- unit_converter: å•ä½è½¬æ¢ï¼ˆæ¸©åº¦ã€é•¿åº¦ç­‰ï¼‰
- data_comparison: æ•°æ®æ¯”è¾ƒï¼ˆæœ€å¤§æœ€å°å€¼ã€æ’åºï¼‰
- logic_reasoning: é€»è¾‘æ¨ç†è¾…åŠ©
- library_system: å›¾ä¹¦é¦†ç®¡ç†ç³»ç»Ÿï¼ˆJSONæŸ¥è¯¢ï¼‰
- end_conversation_detector: å¯¹è¯ç»“æŸæ£€æµ‹
- web_search: ç½‘ç»œæœç´¢ï¼ˆæ¨¡å‹è‡ªä¸»å†³å®šæœç´¢è¯ï¼‰
- file_operation: æ–‡ä»¶æ“ä½œï¼ˆæ¨¡å‹è‡ªä¸»å†³å®šæ“ä½œç±»å‹ï¼‰
- set_reminder: æé†’è®¾ç½®ï¼ˆæ¨¡å‹è‡ªä¸»æå–ä»»åŠ¡å’Œæ—¶é—´ï¼‰

âš ï¸ é‡è¦è§„åˆ™ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰ï¼š
1. **æ•°å­¦è®¡ç®—å¿…é¡»è°ƒç”¨calculator** - å³ä½¿ç®€å•å¦‚"1+1"
2. **æ—¶é—´æŸ¥è¯¢å¿…é¡»è°ƒç”¨time_tool** - ä¸è¦çŒœæµ‹
3. **æ–‡æœ¬ç»Ÿè®¡å¿…é¡»è°ƒç”¨text_analyzer** - ä¸è¦ä¼°ç®—
4. **å•ä½è½¬æ¢å¿…é¡»è°ƒç”¨unit_converter** - ä¸è¦å¿ƒç®—
5. **å¯¹è¯ç»“æŸå¿…é¡»è°ƒç”¨end_conversation_detector** - æ£€æµ‹åˆ°"å†è§"ç­‰å…³é”®è¯æ—¶å¼ºåˆ¶è°ƒç”¨

ğŸ”„ æ¨ç†æµç¨‹ï¼š
ç¬¬1æ­¥ï¼šåˆ†æç”¨æˆ·é—®é¢˜ç±»å‹å’Œæ„å›¾
ç¬¬2æ­¥ï¼šç¡®å®šéœ€è¦ä½¿ç”¨çš„å·¥å…·ï¼ˆæ ¹æ®ä¸Šè¿°è§„åˆ™ï¼‰
ç¬¬3æ­¥ï¼šè‡ªä¸»å†³å®šå·¥å…·çš„è¾“å…¥å‚æ•°
ç¬¬4æ­¥ï¼šç­‰å¾…å·¥å…·æ‰§è¡Œç»“æœ
ç¬¬5æ­¥ï¼šåŸºäºç»“æœè¿›è¡Œæ¨ç†å¹¶ç”Ÿæˆç­”æ¡ˆ

ğŸ’¡ ç¤ºä¾‹ï¼š
ç”¨æˆ·ï¼š"è®¡ç®—sqrt(2)ä¿ç•™3ä½å°æ•°"
â†’ åˆ†æï¼šè¿™æ˜¯æ•°å­¦è®¡ç®—é—®é¢˜
â†’ å†³ç­–ï¼šå¿…é¡»ä½¿ç”¨calculatorå·¥å…·
â†’ å‚æ•°ï¼šexpression="round(sqrt(2), 3)"
â†’ æ‰§è¡Œï¼šè°ƒç”¨å·¥å…·
â†’ å›ç­”ï¼šåŸºäºç»“æœå›ç­”ç”¨æˆ·

ç”¨æˆ·ï¼š"å†è§"
â†’ åˆ†æï¼šåŒ…å«ç»“æŸå…³é”®è¯æˆ–è€…ç›¸å…³ç»“æŸè¯
â†’ å†³ç­–ï¼šå¿…é¡»è°ƒç”¨end_conversation_detector
â†’ å‚æ•°ï¼šuser_message="å†è§"
â†’ æ‰§è¡Œï¼šæ£€æµ‹ç»“æœ
â†’ å›ç­”ï¼šå‘Šåˆ«è¯­

ç°åœ¨å¼€å§‹ä¸¥æ ¼éµå®ˆè§„åˆ™ï¼Œå¸®åŠ©ç”¨æˆ·ï¼"""
    
    def _convert_tools_to_openai_format(self) -> List[Dict]:
        """
        å°†LangChainå·¥å…·è½¬æ¢ä¸ºOpenAI Function Callingæ ¼å¼
        
        è¿™æ˜¯æ··åˆæ¶æ„çš„å…³é”®ï¼šä¿ç•™LangChainå·¥å…·å®šä¹‰ï¼Œä½†ç”¨OpenAIæ ¼å¼è°ƒç”¨
        """
        openai_tools = []
        
        for tool in self.langchain_tools:
            # æå–å‚æ•°schema
            if hasattr(tool, 'args_schema') and tool.args_schema:
                # ä½¿ç”¨model_json_schemaæ›¿ä»£deprecatedçš„schemaæ–¹æ³•
                parameters = tool.args_schema.model_json_schema()
            else:
                parameters = {
                    "type": "object",
                    "properties": {},
                    "required": []
                }
            
            # è½¬æ¢ä¸ºOpenAIæ ¼å¼
            openai_tool = {
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": parameters
                }
            }
            
            openai_tools.append(openai_tool)
        
        return openai_tools
    
    def _execute_tool(self, tool_name: str, arguments: Dict) -> str:
        """
        æ‰§è¡ŒLangChainå·¥å…·
        
        Args:
            tool_name: å·¥å…·åç§°
            arguments: å·¥å…·å‚æ•°
            
        Returns:
            å·¥å…·æ‰§è¡Œç»“æœ
        """
        if tool_name not in self.tool_map:
            return f"é”™è¯¯ï¼šå·¥å…· '{tool_name}' ä¸å­˜åœ¨"
        
        try:
            tool = self.tool_map[tool_name]
            result = tool._run(**arguments)
            return str(result)
        except Exception as e:
            return f"å·¥å…·æ‰§è¡Œé”™è¯¯: {str(e)}"
    
    def _check_end_keywords(self, user_input: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«ç»“æŸå…³é”®è¯"""
        end_keywords = [
            'å†è§', 'æ‹œæ‹œ', 'bye', 'goodbye', 'é€€å‡º', 'ç»“æŸ',
            'å…³é—­', 'ç¦»å¼€', 'ä¸èŠäº†', 'èµ°äº†', 'quit', 'exit',
            '886', '88', 'ä¸‹çº¿', 'æ–­å¼€'
        ]
        user_lower = user_input.lower().strip()
        return any(keyword in user_lower for keyword in end_keywords)
    
    def run(self, user_input: str, show_reasoning: bool = True) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ¨ç†ï¼ˆéæµå¼ï¼‰
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            show_reasoning: æ˜¯å¦æ˜¾ç¤ºæ¨ç†è¿‡ç¨‹
            
        Returns:
            æ‰§è¡Œç»“æœ
        """
        if show_reasoning:
            print("\n" + "="*70)
            print("æ··åˆæ¶æ„æ¨ç†è¿‡ç¨‹ï¼ˆOpenAIåŸç”Ÿ + LangChainå·¥å…·ï¼‰")
            print("="*70)
        
        # æ£€æµ‹ç»“æŸå…³é”®è¯
        contains_end_keyword = self._check_end_keywords(user_input)
        if contains_end_keyword and show_reasoning:
            print(f"\né¢„å¤„ç†ï¼šæ£€æµ‹åˆ°ç»“æŸå…³é”®è¯ï¼Œå°†å¼ºåˆ¶è¦æ±‚è°ƒç”¨end_conversation_detector")
        
        # æ„å»ºæ¶ˆæ¯ï¼ˆåˆ©ç”¨KV Cacheï¼‰
        messages = self._build_messages(user_input, contains_end_keyword)
        
        # æ¨ç†æ­¥éª¤è®°å½•
        reasoning_steps = []
        tool_call_count = 0
        
        try:
            # ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼šæ¨¡å‹å†³ç­–
            if show_reasoning:
                print(f"\n{'â”€'*70}")
                print("è°ƒç”¨OpenAI APIè¿›è¡Œæ¨ç†...")
                print(f"{'â”€'*70}")
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                tools=self.openai_tools,
                tool_choice="auto",  # å¯ä»¥æ”¹ä¸º"required"å¼ºåˆ¶è°ƒç”¨å·¥å…·
                temperature=self.temperature
            )
            
            assistant_message = response.choices[0].message
            
            # å¤„ç†å·¥å…·è°ƒç”¨
            if assistant_message.tool_calls:
                if show_reasoning:
                    print(f"\nâœ… æ¨¡å‹å†³å®šè°ƒç”¨å·¥å…·ï¼ˆå…±{len(assistant_message.tool_calls)}ä¸ªï¼‰")
                
                # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°å†å²
                messages.append({
                    "role": "assistant",
                    "content": assistant_message.content or "",
                    "tool_calls": [
                        {
                            "id": tc.id,
                            "type": "function",
                            "function": {
                                "name": tc.function.name,
                                "arguments": tc.function.arguments
                            }
                        } for tc in assistant_message.tool_calls
                    ]
                })
                
                # æ‰§è¡Œæ¯ä¸ªå·¥å…·è°ƒç”¨
                for tool_call in assistant_message.tool_calls:
                    tool_call_count += 1
                    tool_name = tool_call.function.name
                    arguments = json.loads(tool_call.function.arguments)
                    
                    if show_reasoning:
                        self._display_tool_call(
                            tool_call_count,
                            tool_name,
                            arguments
                        )
                    
                    # æ‰§è¡ŒLangChainå·¥å…·
                    result = self._execute_tool(tool_name, arguments)
                    
                    if show_reasoning:
                        self._display_tool_result(result)
                    
                    # è®°å½•æ¨ç†æ­¥éª¤
                    reasoning_steps.append({
                        'step': tool_call_count,
                        'tool': tool_name,
                        'arguments': arguments,
                        'result': result
                    })
                    
                    # æ·»åŠ å·¥å…·ç»“æœåˆ°æ¶ˆæ¯
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "content": result
                    })
                
                # ç¬¬äºŒæ¬¡è°ƒç”¨ï¼šåŸºäºå·¥å…·ç»“æœç”Ÿæˆæœ€ç»ˆå›ç­”
                if show_reasoning:
                    print(f"\n{'â”€'*70}")
                    print("ğŸ’­ æ¨¡å‹åŸºäºå·¥å…·ç»“æœç”Ÿæˆæœ€ç»ˆå›ç­”...")
                    print(f"{'â”€'*70}")
                
                final_response = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    temperature=self.temperature
                )
                
                final_answer = final_response.choices[0].message.content
            else:
                # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥å›ç­”
                if show_reasoning:
                    print("\nâš ï¸  æ¨¡å‹é€‰æ‹©ç›´æ¥å›ç­”ï¼ˆæœªè°ƒç”¨å·¥å…·ï¼‰")
                final_answer = assistant_message.content
            
            # æ›´æ–°å¯¹è¯å†å²ï¼ˆç”¨äºKV Cacheï¼‰
            if self.enable_cache:
                self.conversation_history.append({
                    "role": "user",
                    "content": user_input
                })
                self.conversation_history.append({
                    "role": "assistant",
                    "content": final_answer
                })
            
            # åˆ†å‰²å¥å­ï¼ˆä¸ºTTSå‡†å¤‡ï¼‰
            sentences = self._split_sentences(final_answer)
            
            if show_reasoning:
                print(f"\n{'='*70}")
                print("ğŸ’¬ æœ€ç»ˆå›ç­”ï¼ˆå·²åˆ†å¥ï¼‰")
                print(f"{'='*70}")
                for i, sent in enumerate(sentences, 1):
                    print(f"{i}. {sent}")
                print(f"{'='*70}\n")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç»“æŸå¯¹è¯
            should_end = any(
                step['tool'] == 'end_conversation_detector' and 
                'END_CONVERSATION' in step['result']
                for step in reasoning_steps
            )
            
            return {
                'success': True,
                'output': final_answer,
                'sentences': sentences,
                'reasoning_steps': reasoning_steps,
                'tool_calls': tool_call_count,
                'should_end': should_end,
                'cached_tokens': len(self.conversation_history) if self.enable_cache else 0
            }
            
        except Exception as e:
            error_msg = f"æ‰§è¡Œé”™è¯¯: {str(e)}"
            print(f"\nâŒ {error_msg}")
            return {
                'success': False,
                'output': error_msg,
                'error': str(e)
            }
    
    def _build_messages(self, user_input: str, force_end_detection: bool = False) -> List[Dict]:
        """
        æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        
        ç³»ç»Ÿæç¤ºè¯ä¼šè¢«OpenAIè‡ªåŠ¨ç¼“å­˜ï¼ˆPrompt Cachingï¼‰
        å¯¹è¯å†å²ä¹Ÿä¼šè¢«KV Cacheä¼˜åŒ–
        """
        messages = [
            {"role": "system", "content": self.system_prompt}
        ]
        
        # æ·»åŠ å¯¹è¯å†å²ï¼ˆKV Cacheä¼˜åŒ–ï¼‰
        if self.enable_cache:
            messages.extend(self.conversation_history)
        
        # æ·»åŠ å½“å‰è¾“å…¥
        user_message = user_input
        if force_end_detection:
            user_message += "\n\n[ç³»ç»Ÿè¦æ±‚ï¼šæ£€æµ‹åˆ°ç»“æŸå…³é”®è¯ï¼Œå¿…é¡»è°ƒç”¨end_conversation_detectorå·¥å…·]"
        
        messages.append({
            "role": "user",
            "content": user_message
        })
        
        return messages
    
    def _display_tool_call(self, step: int, tool_name: str, arguments: Dict):
        """æ˜¾ç¤ºå·¥å…·è°ƒç”¨ä¿¡æ¯"""
        print(f"\n{'='*70}")
        print(f"ğŸ“ æ¨ç†æ­¥éª¤ {step}")
        print(f"{'='*70}")
        print(f"\nâœ… æ¨¡å‹å†³ç­–:")
        print(f"   â†’ é€‰æ‹©å·¥å…·: {tool_name}")
        print(f"\nğŸ“¥ æ¨¡å‹å†³å®šçš„å‚æ•°:")
        print(f"{'â”€'*70}")
        formatted_args = json.dumps(arguments, ensure_ascii=False, indent=6)
        for line in formatted_args.split('\n'):
            print(f"   {line}")
        print(f"{'â”€'*70}")
    
    def _display_tool_result(self, result: str):
        """æ˜¾ç¤ºå·¥å…·æ‰§è¡Œç»“æœ"""
        print(f"\nğŸ“¤ å·¥å…·è¿”å›ç»“æœ:")
        print(f"{'â”€'*70}")
        if len(result) > 500:
            print(f"   {result[:500]}...")
            print(f"   ... (ç»“æœè¿‡é•¿ï¼Œå·²æˆªæ–­)")
        else:
            for line in result.split('\n'):
                print(f"   {line}")
        print(f"{'â”€'*70}")
    
    def _split_sentences(self, text: str) -> List[str]:
        """æŒ‰å¥å­åˆ†å‰²æ–‡æœ¬ï¼ˆä¸ºTTSå‡†å¤‡ï¼‰"""
        pattern = r'([^ã€‚ï¼ï¼Ÿ.!?]+[ã€‚ï¼ï¼Ÿ.!?]+)'
        sentences = re.findall(pattern, text)
        
        if not sentences:
            pattern = r'([^ï¼Œ,;ï¼›]+[ï¼Œ,;ï¼›]+)'
            sentences = re.findall(pattern, text)
        
        if not sentences:
            sentences = [text]
        
        return [s.strip() for s in sentences if s.strip()]
    
    def _is_json_result(self, result: str) -> bool:
        """æ£€æŸ¥ç»“æœæ˜¯å¦æ˜¯JSONæ ¼å¼"""
        try:
            json.loads(result)
            return True
        except:
            return False
    
    def run_with_tts(self, 
                     user_input: str, 
                     show_reasoning: bool = True,
                     simulate_mode: bool = True) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ¨ç†å¹¶æ’­æ”¾TTSéŸ³é¢‘
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            show_reasoning: æ˜¯å¦æ˜¾ç¤ºæ¨ç†è¿‡ç¨‹
            simulate_mode: æ˜¯å¦æ¨¡æ‹Ÿæ¨¡å¼ï¼ˆæ— çœŸå®TTSå¼•æ“æ—¶ï¼‰
            
        Returns:
            åŒ…å« raw_output, tts_chunks, playback_success çš„å­—å…¸
        """
        if not self.enable_tts:
            print("âš ï¸  TTSæœªå¯ç”¨ï¼Œä½¿ç”¨æ™®é€šæ¨¡å¼")
            return self.run(user_input, show_reasoning)
        
        # è¯­éŸ³åé¦ˆï¼šå¼€å§‹æ€è€ƒ
        if self.voice_mode:
            self.voice_feedback.start('thinking')
        
        # æ‰§è¡Œæ¨ç†
        result = self.run(user_input, show_reasoning)
        
        # åœæ­¢è¯­éŸ³åé¦ˆ
        if self.voice_mode:
            self.voice_feedback.stop()
        
        if not result['success']:
            return result
        
        # TTSä¼˜åŒ–å¹¶æ’­æ”¾
        print(f"\n{'='*70}")
        print("ğŸµ TTSéŸ³é¢‘æ’­æ”¾")
        print(f"{'='*70}\n")
        
        tts_result = self.tts_optimizer.optimize_and_play(
            text=result['output'],
            simulate_mode=simulate_mode
        )
        
        # åˆå¹¶ç»“æœ
        result.update({
            'tts_chunks': tts_result.get('tts_chunks', []),
            'tts_success': tts_result.get('success', False),
            'total_tts_chunks': tts_result.get('total_chunks', 0)
        })
        
        return result
    
    def run_with_tts_demo(self, 
                          user_input: str,
                          show_text_and_tts: bool = True) -> Dict[str, Any]:
        """
        æ¼”ç¤ºæ¨¡å¼ï¼šåŒæ—¶æ˜¾ç¤ºæ–‡æœ¬è¾“å‡ºå’ŒTTSç»“æ„
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            show_text_and_tts: æ˜¯å¦æ˜¾ç¤ºåŒè½¨è¾“å‡º
            
        Returns:
            å®Œæ•´ç»“æœå­—å…¸
        """
        if not self.enable_tts:
            print("âš ï¸  TTSæœªå¯ç”¨")
            return self.run(user_input, show_reasoning=True)
        
        # è¯­éŸ³åé¦ˆ
        if self.voice_mode:
            self.voice_feedback.start('thinking')
        
        # æ‰§è¡Œæ¨ç†
        result = self.run(user_input, show_reasoning=True)
        
        # åœæ­¢è¯­éŸ³åé¦ˆ
        if self.voice_mode:
            self.voice_feedback.stop()
        
        if not result['success']:
            return result
        
        # TTSä¼˜åŒ–ï¼ˆä»…ä¼˜åŒ–æ–‡æœ¬ï¼Œä¸æ’­æ”¾ï¼‰
        tts_chunks = self.tts_optimizer.optimize_text_only(result['output'])
        
        if show_text_and_tts:
            # æ˜¾ç¤ºåŒè½¨è¾“å‡º
            print(f"\n{'='*70}")
            print("ğŸ“ LLMåŸå§‹æ–‡æœ¬è¾“å‡º")
            print(f"{'='*70}")
            print(result['output'])
            
            if tts_chunks:
                print(f"\n{'='*70}")
                print("ğŸ—£ï¸  TTSä¼˜åŒ–è¾“å‡ºç»“æ„")
                print(f"{'='*70}\n")
                
                for chunk in tts_chunks:
                    print(f"[Chunk {chunk['chunk_id']}]")
                    print(f"  æ–‡æœ¬: {chunk['text']}")
                    print(f"  é•¿åº¦: {chunk['length']} å­—ç¬¦")
                    print(f"  åœé¡¿: {chunk['pause_after']}ms")
                    print()
                
                print(f"{'='*70}")
                print(f"ğŸ“Š TTSç»Ÿè®¡: å…±{len(tts_chunks)}ä¸ªåˆ†æ®µ")
                total_pause = sum(c['pause_after'] for c in tts_chunks)
                print(f"   é¢„è®¡æ’­æ”¾æ—¶é•¿: ~{total_pause/1000:.1f}ç§’ï¼ˆä¸å«è¯­éŸ³ï¼‰")
                print(f"{'='*70}\n")
        
        result['tts_chunks'] = tts_chunks
        result['total_tts_chunks'] = len(tts_chunks)
        
        return result
    
    def run_with_streaming_tts(self,
                                user_input: str,
                                show_reasoning: bool = True,
                                tts_wait_timeout: int = 30) -> Dict[str, Any]:
        """
        æµå¼TTSæ¨¡å¼ï¼šLLMæµå¼è¾“å‡º â†’ å®æ—¶TTSæ’­æ”¾
        
        æ¶æ„ï¼š
            LLM Streaming â†’ Smart Splitter â†’ TTS Generator â†’ Audio Player
                              â†“ èƒŒå‹           â†“ èƒŒå‹          â†“
                           æ–‡æœ¬é˜Ÿåˆ—          éŸ³é¢‘é˜Ÿåˆ—        æ’­æ”¾é˜Ÿåˆ—
        
        ç‰¹ç‚¹ï¼š
        1. æ›´ä½å»¶è¿Ÿ - è¾¹ç”Ÿæˆè¾¹æ’­æ”¾
        2. èµ„æºå¯æ§ - æœ‰ç•Œé˜Ÿåˆ—é˜²æ­¢çˆ†ç‚¸
        3. è‡ªåŠ¨èƒŒå‹ - ä¿æŠ¤ç³»ç»Ÿç¨³å®š
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            show_reasoning: æ˜¯å¦æ˜¾ç¤ºæ¨ç†è¿‡ç¨‹
            
        Returns:
            å®Œæ•´ç»“æœå­—å…¸
        """
        if not self.enable_streaming_tts:
            print("âš ï¸  æµå¼TTSæœªå¯ç”¨ï¼Œä½¿ç”¨æ™®é€šæ¨¡å¼")
            return self.run(user_input, show_reasoning=show_reasoning)
        
        print(f"\n{'='*70}")
        print("âš¡ æµå¼TTSæ¨¡å¼")
        print(f"{'='*70}\n")
        
        # å¯åŠ¨æµå¼ç®¡é“
        self.streaming_pipeline.start()
        
        # è¯­éŸ³åé¦ˆ
        if self.voice_mode:
            self.voice_feedback.start('thinking')
        
        try:
            # === é˜¶æ®µ1ï¼šLLMæ¨ç†ï¼ˆä½¿ç”¨OpenAI Stream APIï¼‰===
            print(f"ğŸ§  LLMæ¨ç†ä¸­...\n")
            
            # æ„å»ºæ¶ˆæ¯
            messages = [
                {"role": "system", "content": self.system_prompt},
                *self.conversation_history,
                {"role": "user", "content": user_input}
            ]
            
            # è°ƒç”¨OpenAIæµå¼API
            stream = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                tools=self.openai_tools,
                tool_choice="auto",
                temperature=self.temperature,
                stream=True  # å¯ç”¨æµå¼è¾“å‡º
            )
            
            # ç´¯ç§¯å˜é‡
            full_response = ""
            tool_calls_buffer = []
            current_tool_call = None
            
            # === é˜¶æ®µ2ï¼šæµå¼å¤„ç†LLMè¾“å‡º ===
            for chunk in stream:
                delta = chunk.choices[0].delta
                
                # å¤„ç†æ–‡æœ¬å†…å®¹
                if delta.content:
                    content_piece = delta.content
                    full_response += content_piece
                    
                    # å®æ—¶é€å…¥TTSç®¡é“ï¼ˆæ™ºèƒ½åˆ†å¥ä¼šè‡ªåŠ¨å¤„ç†ï¼‰
                    self.streaming_pipeline.add_text_from_llm(content_piece)
                    
                    if show_reasoning:
                        print(content_piece, end='', flush=True)
                
                # å¤„ç†å·¥å…·è°ƒç”¨
                if delta.tool_calls:
                    for tool_call_delta in delta.tool_calls:
                        if tool_call_delta.index is not None:
                            # æ–°çš„å·¥å…·è°ƒç”¨
                            if current_tool_call is None or \
                               tool_call_delta.index != current_tool_call.get('index'):
                                if current_tool_call:
                                    tool_calls_buffer.append(current_tool_call)
                                current_tool_call = {
                                    'index': tool_call_delta.index,
                                    'id': tool_call_delta.id or '',
                                    'type': 'function',
                                    'function': {
                                        'name': tool_call_delta.function.name or '',
                                        'arguments': tool_call_delta.function.arguments or ''
                                    }
                                }
                            else:
                                # ç´¯ç§¯å·¥å…·è°ƒç”¨å‚æ•°
                                if tool_call_delta.function.arguments:
                                    current_tool_call['function']['arguments'] += \
                                        tool_call_delta.function.arguments
            
            # æ·»åŠ æœ€åä¸€ä¸ªå·¥å…·è°ƒç”¨
            if current_tool_call:
                tool_calls_buffer.append(current_tool_call)
            
            # åœæ­¢è¯­éŸ³åé¦ˆ
            if self.voice_mode:
                self.voice_feedback.stop()
            
            print(f"\n")
            
            # === é˜¶æ®µ3ï¼šå¤„ç†å·¥å…·è°ƒç”¨ ===
            should_end = False  # æ£€æµ‹å¯¹è¯ç»“æŸ
            
            if tool_calls_buffer:
                if show_reasoning:
                    print(f"\n{'='*70}")
                    print("ğŸ› ï¸  å·¥å…·è°ƒç”¨")
                    print(f"{'='*70}\n")
                
                tool_messages = []
                for tool_call in tool_calls_buffer:
                    tool_name = tool_call['function']['name']
                    tool_args_str = tool_call['function']['arguments']
                    
                    # è§£æå‚æ•°ï¼ˆä¿®å¤bugï¼šå¿…é¡»è½¬ä¸ºå­—å…¸ï¼‰
                    try:
                        tool_args = json.loads(tool_args_str)
                    except json.JSONDecodeError as e:
                        tool_args = {}
                        print(f"âš ï¸  å·¥å…·å‚æ•°è§£æå¤±è´¥: {e}")
                    
                    if show_reasoning:
                        print(f"ğŸ“Œ è°ƒç”¨å·¥å…·: {tool_name}")
                        print(f"   å‚æ•°: {tool_args_str}\n")
                    
                    # æ‰§è¡Œå·¥å…·ï¼ˆä¼ å­—å…¸ï¼Œä¸æ˜¯å­—ç¬¦ä¸²ï¼ï¼‰
                    tool_result = self._execute_tool(tool_name, tool_args)
                    
                    # æ£€æµ‹å¯¹è¯ç»“æŸ
                    if tool_name == 'end_conversation_detector' and 'END_CONVERSATION' in tool_result:
                        should_end = True
                    
                    if show_reasoning:
                        print(f"   ç»“æœ: {tool_result}\n")
                    
                    # æ„å»ºå·¥å…·æ¶ˆæ¯
                    tool_messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call['id'],
                        "content": str(tool_result)
                    })
                
                # === é˜¶æ®µ4ï¼šè·å–æœ€ç»ˆå›å¤ï¼ˆå¸¦å·¥å…·ç»“æœï¼‰===
                print(f"{'='*70}")
                print("ğŸ’¬ æœ€ç»ˆå›å¤")
                print(f"{'='*70}\n")
                
                # æ„å»ºåŒ…å«å·¥å…·ç»“æœçš„æ¶ˆæ¯
                messages_with_tools = messages + [
                    {
                        "role": "assistant",
                        "content": full_response or None,
                        "tool_calls": [
                            {
                                "id": tc['id'],
                                "type": "function",
                                "function": {
                                    "name": tc['function']['name'],
                                    "arguments": tc['function']['arguments']
                                }
                            } for tc in tool_calls_buffer
                        ]
                    }
                ] + tool_messages
                
                # å†æ¬¡è°ƒç”¨ï¼ˆæµå¼ï¼‰
                final_stream = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages_with_tools,
                    temperature=self.temperature,
                    stream=True
                )
                
                final_response = ""
                for chunk in final_stream:
                    delta = chunk.choices[0].delta
                    if delta.content:
                        content_piece = delta.content
                        final_response += content_piece
                        
                        # å®æ—¶é€å…¥TTSç®¡é“
                        self.streaming_pipeline.add_text_from_llm(content_piece)
                        
                        if show_reasoning:
                            print(content_piece, end='', flush=True)
                
                print(f"\n")
                full_response = final_response
            
            # === é˜¶æ®µ5ï¼šåˆ·æ–°TTSç®¡é“ï¼Œå¤„ç†å‰©ä½™æ–‡æœ¬ ===
            self.streaming_pipeline.flush_remaining_text()
            
            # === é˜¶æ®µ6ï¼šç­‰å¾…æ‰€æœ‰éŸ³é¢‘æ’­æ”¾å®Œæˆ ===
            print(f"\n{'='*70}")
            print("ğŸµ ç­‰å¾…éŸ³é¢‘æ’­æ”¾å®Œæˆ...")
            print(f"{'='*70}\n")
            
            import time
            start_wait = time.time()
            
            while True:
                stats = self.streaming_pipeline.get_stats()
                
                # ğŸ”§ æ—¥å¿—ï¼šæ˜¾ç¤ºå½“å‰çŠ¶æ€
                self.logger.debug(
                    f"â³ TTSçŠ¶æ€ - æ–‡æœ¬é˜Ÿåˆ—:{stats.text_queue_size} "
                    f"éŸ³é¢‘é˜Ÿåˆ—:{stats.audio_queue_size} "
                    f"æ´»è·ƒä»»åŠ¡:{stats.active_tasks} "
                    f"æ’­æ”¾ä¸­:{stats.is_playing}"
                )
                
                # æ£€æŸ¥æ‰€æœ‰æ¡ä»¶ï¼ˆå…³é”®ï¼šåŒ…æ‹¬ is_playingï¼‰
                all_done = (
                    stats.text_queue_size == 0 and 
                    stats.audio_queue_size == 0 and 
                    stats.active_tasks == 0 and
                    not stats.is_playing  # å…³é”®ï¼šç¡®ä¿æ²¡æœ‰éŸ³é¢‘æ­£åœ¨æ’­æ”¾ï¼
                )
                
                if all_done:
                    self.logger.info("âœ… TTS æ’­æ”¾å®Œæˆ")
                    break
                
                # ğŸ”§ è¶…æ—¶ä¿æŠ¤
                elapsed = time.time() - start_wait
                if elapsed > tts_wait_timeout:
                    self.logger.warning(
                        f"âš ï¸  TTS ç­‰å¾…è¶…æ—¶ ({tts_wait_timeout}ç§’)ï¼Œå¼ºåˆ¶ç»§ç»­\n"
                        f"   çŠ¶æ€: æ–‡æœ¬é˜Ÿåˆ—={stats.text_queue_size}, "
                        f"éŸ³é¢‘é˜Ÿåˆ—={stats.audio_queue_size}, "
                        f"æ´»è·ƒä»»åŠ¡={stats.active_tasks}, "
                        f"æ’­æ”¾ä¸­={stats.is_playing}"
                    )
                    break
                
                time.sleep(0.5)
            
            # åœæ­¢ç®¡é“ï¼ˆğŸ”§ æ³¨é‡Šæ‰ï¼Œä¿æŒç®¡é“è¿è¡Œä»¥æ”¯æŒå¤šè½®å¯¹è¯ï¼‰
            # self.streaming_pipeline.stop(wait=True, timeout=5.0)
            
            # === é˜¶æ®µ7ï¼šæ›´æ–°å¯¹è¯å†å² ===
            if self.enable_cache:
                self.conversation_history.append({"role": "user", "content": user_input})
                self.conversation_history.append({"role": "assistant", "content": full_response})
            
            # è¿”å›ç»“æœ
            return {
                'success': True,
                'input': user_input,
                'output': full_response,
                'tool_calls': len(tool_calls_buffer) if tool_calls_buffer else 0,
                'streaming_stats': self.streaming_pipeline.get_stats().to_dict(),
                'should_end': should_end  # æ·»åŠ å¯¹è¯ç»“æŸæ ‡å¿—
            }
            
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            
            # ç¡®ä¿ç®¡é“åœæ­¢
            if self.streaming_pipeline:
                self.streaming_pipeline.stop(wait=False)
            
            return {
                'success': False,
                'error': str(e),
                'input': user_input
            }
    
    def clear_cache(self):
        """æ¸…é™¤å¯¹è¯å†å²ç¼“å­˜"""
        self.conversation_history = []
        print("âœ… å¯¹è¯å†å²å·²æ¸…é™¤ï¼ˆKV Cacheé‡ç½®ï¼‰")
    
    def get_cache_stats(self) -> Dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'conversation_turns': len(self.conversation_history) // 2,
            'total_messages': len(self.conversation_history),
            'estimated_cached_tokens': sum(
                len(msg['content']) // 4 
                for msg in self.conversation_history
            ),
            'system_prompt_tokens': len(self.system_prompt) // 4
        }


# å¿«é€Ÿæµ‹è¯•
if __name__ == "__main__":
    print("\n" + "="*80)
    print("ğŸš€ æ··åˆæ¶æ„Agentæµ‹è¯•")
    print("="*80)
    
    agent = HybridReasoningAgent()
    
    # æµ‹è¯•1ï¼šæ•°å­¦è®¡ç®—
    print("\nã€æµ‹è¯•1ã€‘æ•°å­¦è®¡ç®—")
    result1 = agent.run("è®¡ç®—sqrt(2)ä¿ç•™3ä½å°æ•°")
    
    # æµ‹è¯•2ï¼šå¯¹è¯ç»“æŸ
    print("\nã€æµ‹è¯•2ã€‘å¯¹è¯ç»“æŸæ£€æµ‹")
    result2 = agent.run("å¥½çš„ï¼Œå†è§ï¼")
    
    # ç¼“å­˜ç»Ÿè®¡
    print("\nã€ç¼“å­˜ç»Ÿè®¡ã€‘")
    stats = agent.get_cache_stats()
    print(f"å¯¹è¯è½®æ¬¡: {stats['conversation_turns']}")
    print(f"ç¼“å­˜tokensä¼°è®¡: ~{stats['estimated_cached_tokens']}")
    print(f"ç³»ç»Ÿæç¤ºè¯tokens: ~{stats['system_prompt_tokens']} (å·²ç¼“å­˜)")

