"""
OpenAI Whisper ASR æ¥å£
æ”¯æŒæœ€é«˜è´¨é‡çš„è¯­éŸ³è¯†åˆ«ï¼Œè‡ªåŠ¨è¯­è¨€æ£€æµ‹
"""
import openai
import time
from pathlib import Path
from typing import Optional, Dict, Any, List
from dataclasses import dataclass
from logger_config import get_logger
import config

logger = get_logger(__name__)


@dataclass
class ASRResult:
    """ASR è¯†åˆ«ç»“æœ"""
    text: str                           # è¯†åˆ«çš„æ–‡æœ¬
    language: str                       # æ£€æµ‹åˆ°çš„è¯­è¨€
    duration: float                     # éŸ³é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰
    processing_time: float              # å¤„ç†è€—æ—¶ï¼ˆç§’ï¼‰
    confidence: Optional[float] = None  # ç½®ä¿¡åº¦ï¼ˆå¦‚æœæœ‰ï¼‰
    segments: Optional[List[Dict]] = None  # æ—¶é—´æˆ³åˆ†æ®µï¼ˆå¦‚æœè¯·æ±‚ï¼‰


class OpenAIASR:
    """
    OpenAI Whisper ASR æ¥å£ - æœ€é«˜è´¨é‡è¯­éŸ³è¯†åˆ«
    
    ç‰¹æ€§ï¼š
    - âœ… è‡ªåŠ¨è¯­è¨€æ£€æµ‹ï¼ˆ98+ ç§è¯­è¨€ï¼‰
    - âœ… è¶…é«˜å‡†ç¡®ç‡ï¼ˆState-of-the-artï¼‰
    - âœ… æ”¯æŒå™ªéŸ³ç¯å¢ƒ
    - âœ… æ”¯æŒå¤šç§éŸ³é¢‘æ ¼å¼
    - âœ… å¯é€‰æ—¶é—´æˆ³åˆ†æ®µ
    - âœ… ä¸“ä¸šæœ¯è¯­ä¼˜åŒ–ï¼ˆpromptï¼‰
    
    æ”¯æŒçš„éŸ³é¢‘æ ¼å¼ï¼š
    - MP3, MP4, MPEG, MPGA, M4A, WAV, WEBM
    
    æœ€å¤§æ–‡ä»¶å¤§å°ï¼š25 MB
    """
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        model: str = "whisper-1",
        temperature: float = 0.0,
        timeout: int = 30
    ):
        """
        åˆå§‹åŒ– ASR
        
        Args:
            api_key: OpenAI API Keyï¼ˆNone=ä» config è¯»å–ï¼‰
            model: æ¨¡å‹åç§°ï¼ˆwhisper-1 æ˜¯ç›®å‰æœ€å¥½çš„ï¼‰
            temperature: æ¸©åº¦ï¼ˆ0=æœ€ç¡®å®šï¼Œ0.2=ç¨å¾®éšæœºï¼‰
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.api_key = api_key or config.OPENAI_API_KEY
        self.model = model
        self.temperature = temperature
        self.timeout = timeout
        
        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
        self.client = openai.OpenAI(
            api_key=self.api_key,
            timeout=timeout
        )
        
        logger.info(f"âœ… ASR åˆå§‹åŒ–æˆåŠŸ")
        logger.info(f"   æ¨¡å‹: {model}")
        logger.info(f"   è¯­è¨€æ£€æµ‹: è‡ªåŠ¨ï¼ˆ98+ ç§è¯­è¨€ï¼‰")
        logger.info(f"   æ¸©åº¦: {temperature}")
    
    def transcribe(
        self,
        audio_file: str,
        language: Optional[str] = None,
        prompt: Optional[str] = None,
        return_segments: bool = False,
        verbose: bool = True
    ) -> ASRResult:
        """
        è¯­éŸ³è¯†åˆ«ï¼ˆåŒæ­¥ï¼‰
        
        Args:
            audio_file: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            language: è¯­è¨€ä»£ç ï¼ˆNone=è‡ªåŠ¨æ£€æµ‹ï¼Œæ¨èï¼‰
            prompt: æç¤ºè¯ï¼ˆæå‡ä¸“ä¸šæœ¯è¯­è¯†åˆ«å‡†ç¡®ç‡ï¼‰
            return_segments: æ˜¯å¦è¿”å›æ—¶é—´æˆ³åˆ†æ®µ
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        
        Returns:
            ASRResult: è¯†åˆ«ç»“æœ
        
        Example:
            >>> asr = OpenAIASR()
            >>> result = asr.transcribe("audio.mp3")
            >>> print(f"è¯†åˆ«: {result.text}")
            >>> print(f"è¯­è¨€: {result.language}")
            >>> print(f"è€—æ—¶: {result.processing_time:.2f}ç§’")
        """
        start_time = time.time()
        
        try:
            # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨
            audio_path = Path(audio_file)
            if not audio_path.exists():
                raise FileNotFoundError(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_file}")
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ˆOpenAI é™åˆ¶ 25MBï¼‰
            file_size = audio_path.stat().st_size
            if file_size > 25 * 1024 * 1024:
                raise ValueError(f"æ–‡ä»¶è¿‡å¤§: {file_size / 1024 / 1024:.1f}MBï¼ˆæœ€å¤§ 25MBï¼‰")
            
            if verbose:
                logger.info(f"ğŸ§ å¼€å§‹è¯†åˆ«: {audio_path.name} ({file_size / 1024:.1f}KB)")
            
            # æ„å»º API å‚æ•°
            with open(audio_file, "rb") as f:
                kwargs = {
                    "model": self.model,
                    "file": f,
                    "temperature": self.temperature,
                }
                
                # è¯­è¨€ï¼ˆNone=è‡ªåŠ¨æ£€æµ‹ï¼‰
                if language:
                    kwargs["language"] = language
                    if verbose:
                        logger.debug(f"   æŒ‡å®šè¯­è¨€: {language}")
                else:
                    if verbose:
                        logger.debug("   è‡ªåŠ¨æ£€æµ‹è¯­è¨€")
                
                # æç¤ºè¯ï¼ˆæå‡ä¸“ä¸šæœ¯è¯­è¯†åˆ«ï¼‰
                if prompt:
                    kwargs["prompt"] = prompt
                    if verbose:
                        logger.debug(f"   æç¤ºè¯: {prompt[:50]}...")
                
                # è¿”å›æ ¼å¼
                if return_segments:
                    kwargs["response_format"] = "verbose_json"
                    kwargs["timestamp_granularities"] = ["segment"]
                else:
                    kwargs["response_format"] = "verbose_json"  # è·å–è¯­è¨€ä¿¡æ¯
                
                # è°ƒç”¨ OpenAI Whisper API
                response = self.client.audio.transcriptions.create(**kwargs)
            
            processing_time = time.time() - start_time
            
            # è§£æç»“æœ
            if return_segments:
                result = ASRResult(
                    text=response.text,
                    language=response.language,
                    duration=response.duration,
                    processing_time=processing_time,
                    segments=[
                        {
                            'id': seg.id,
                            'start': seg.start,
                            'end': seg.end,
                            'text': seg.text
                        }
                        for seg in response.segments
                    ]
                )
            else:
                result = ASRResult(
                    text=response.text,
                    language=response.language,
                    duration=response.duration,
                    processing_time=processing_time
                )
            
            if verbose:
                logger.info(f"âœ… è¯†åˆ«å®Œæˆ")
                logger.info(f"   æ–‡æœ¬: {result.text[:100]}{'...' if len(result.text) > 100 else ''}")
                logger.info(f"   è¯­è¨€: {result.language}")
                logger.info(f"   éŸ³é¢‘æ—¶é•¿: {result.duration:.2f}ç§’")
                logger.info(f"   å¤„ç†è€—æ—¶: {result.processing_time:.2f}ç§’")
                logger.info(f"   é€Ÿåº¦æ¯”: {result.duration / result.processing_time:.1f}x å®æ—¶")
            
            return result
        
        except FileNotFoundError as e:
            logger.error(f"âŒ æ–‡ä»¶é”™è¯¯: {e}")
            raise
        
        except openai.APIError as e:
            logger.error(f"âŒ OpenAI API é”™è¯¯: {e}")
            raise
        
        except Exception as e:
            logger.error(f"âŒ ASR è¯†åˆ«å¤±è´¥: {e}")
            raise
    
    def transcribe_bytes(
        self,
        audio_bytes: bytes,
        filename: str = "audio.wav",
        language: Optional[str] = None,
        prompt: Optional[str] = None
    ) -> ASRResult:
        """
        ç›´æ¥è¯†åˆ«éŸ³é¢‘å­—èŠ‚æµï¼ˆæ— éœ€ä¿å­˜æ–‡ä»¶ï¼‰
        
        Args:
            audio_bytes: éŸ³é¢‘æ•°æ®
            filename: è™šæ‹Ÿæ–‡ä»¶åï¼ˆç”¨äºæŒ‡å®šæ ¼å¼ï¼‰
            language: è¯­è¨€ä»£ç ï¼ˆNone=è‡ªåŠ¨æ£€æµ‹ï¼‰
            prompt: æç¤ºè¯
        
        Returns:
            ASRResult: è¯†åˆ«ç»“æœ
        """
        from io import BytesIO
        
        start_time = time.time()
        
        try:
            # åˆ›å»ºå­—èŠ‚æµæ–‡ä»¶å¯¹è±¡
            audio_file = BytesIO(audio_bytes)
            audio_file.name = filename
            
            kwargs = {
                "model": self.model,
                "file": audio_file,
                "temperature": self.temperature,
                "response_format": "verbose_json"
            }
            
            if language:
                kwargs["language"] = language
            
            if prompt:
                kwargs["prompt"] = prompt
            
            response = self.client.audio.transcriptions.create(**kwargs)
            
            processing_time = time.time() - start_time
            
            return ASRResult(
                text=response.text,
                language=response.language,
                duration=response.duration,
                processing_time=processing_time
            )
        
        except Exception as e:
            logger.error(f"âŒ å­—èŠ‚æµè¯†åˆ«å¤±è´¥: {e}")
            raise
    
    def transcribe_batch(
        self,
        audio_files: List[str],
        language: Optional[str] = None,
        prompt: Optional[str] = None
    ) -> List[ASRResult]:
        """
        æ‰¹é‡è¯†åˆ«å¤šä¸ªéŸ³é¢‘æ–‡ä»¶
        
        Args:
            audio_files: éŸ³é¢‘æ–‡ä»¶åˆ—è¡¨
            language: è¯­è¨€ä»£ç ï¼ˆNone=è‡ªåŠ¨æ£€æµ‹ï¼‰
            prompt: æç¤ºè¯
        
        Returns:
            è¯†åˆ«ç»“æœåˆ—è¡¨
        """
        logger.info(f"ğŸ“¦ æ‰¹é‡è¯†åˆ«: {len(audio_files)} ä¸ªæ–‡ä»¶")
        
        results = []
        for i, audio_file in enumerate(audio_files, 1):
            logger.info(f"å¤„ç† {i}/{len(audio_files)}: {Path(audio_file).name}")
            
            try:
                result = self.transcribe(
                    audio_file=audio_file,
                    language=language,
                    prompt=prompt,
                    verbose=False
                )
                results.append(result)
            
            except Exception as e:
                logger.error(f"âŒ æ–‡ä»¶ {i} è¯†åˆ«å¤±è´¥: {e}")
                results.append(ASRResult(
                    text="",
                    language="",
                    duration=0,
                    processing_time=0
                ))
        
        logger.info(f"âœ… æ‰¹é‡è¯†åˆ«å®Œæˆ")
        return results
    
    def get_supported_languages(self) -> List[str]:
        """
        è·å–æ”¯æŒçš„è¯­è¨€åˆ—è¡¨
        
        Returns:
            è¯­è¨€ä»£ç åˆ—è¡¨
        """
        # Whisper æ”¯æŒçš„ä¸»è¦è¯­è¨€
        return [
            'zh',  # ä¸­æ–‡
            'en',  # è‹±æ–‡
            'ja',  # æ—¥æ–‡
            'ko',  # éŸ©æ–‡
            'es',  # è¥¿ç­ç‰™è¯­
            'fr',  # æ³•è¯­
            'de',  # å¾·è¯­
            'it',  # æ„å¤§åˆ©è¯­
            'pt',  # è‘¡è„ç‰™è¯­
            'ru',  # ä¿„è¯­
            'ar',  # é˜¿æ‹‰ä¼¯è¯­
            'tr',  # åœŸè€³å…¶è¯­
            'vi',  # è¶Šå—è¯­
            'th',  # æ³°è¯­
            'id',  # å°å°¼è¯­
            # ... è¿˜æœ‰ 80+ ç§è¯­è¨€
        ]
    
    def estimate_cost(self, audio_duration_seconds: float) -> float:
        """
        ä¼°ç®—è¯†åˆ«æˆæœ¬
        
        Args:
            audio_duration_seconds: éŸ³é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰
        
        Returns:
            æˆæœ¬ï¼ˆç¾å…ƒï¼‰
        """
        # Whisper-1 å®šä»·: $0.006 / åˆ†é’Ÿ
        minutes = audio_duration_seconds / 60
        cost = minutes * 0.006
        return cost


# å·¥å‚æ–¹æ³•
def create_asr(
    provider: str = "openai",
    **kwargs
) -> OpenAIASR:
    """
    åˆ›å»º ASR å®ä¾‹
    
    Args:
        provider: ASR æä¾›å•†ï¼ˆç›®å‰åªæ”¯æŒ openaiï¼‰
        **kwargs: å…¶ä»–å‚æ•°
    
    Returns:
        ASR å®ä¾‹
    """
    if provider.lower() == "openai":
        return OpenAIASR(**kwargs)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ ASR æä¾›å•†: {provider}")


